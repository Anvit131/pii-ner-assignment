{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "id": "98da97c5",
      "cell_type": "markdown",
      "source": [
        "# PII NER ‚Äî Beginner-Friendly Colab Notebook\n",
        "\n",
        "This notebook implements the end-to-end PII NER assignment for noisy STT transcripts.\n",
        "\n",
        "**What this notebook contains (Beginner-Friendly):**\n",
        "- Folder setup\n",
        "- Dataset generation (synthetic noisy STT)\n",
        "- Training a DistilBERT token classifier (baseline)\n",
        "- Prediction with conservative post-processing\n",
        "- Simple evaluation and latency measurement\n",
        "- Tips and next steps\n",
        "\n",
        "**Assignment PDF (uploaded):**\n",
        "\n",
        "- `/mnt/data/IIT Madras __ Assignment 2025 (1).pdf` ‚Äî open from the Colab Files pane.\n",
        "\n",
        "---\n",
        "\n",
        "**How to use:** Upload this notebook to Google Colab and run cells sequentially."
      ],
      "metadata": {
        "id": "98da97c5"
      }
    },
    {
      "id": "9cdd63c3",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cdd63c3",
        "outputId": "88556b99-126e-4d9e-f1a5-b1903e6b9cf0"
      },
      "execution_count": null,
      "source": [
        "!mkdir -p data scripts src out\n",
        "!ls -la\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32\n",
            "drwxr-xr-x 1 root root 4096 Nov 23 13:39 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 23 13:38 ..\n",
            "drwxr-xr-x 4 root root 4096 Nov 20 14:30 .config\n",
            "drwxr-xr-x 2 root root 4096 Nov 23 13:39 data\n",
            "drwxr-xr-x 2 root root 4096 Nov 23 13:39 out\n",
            "drwxr-xr-x 1 root root 4096 Nov 20 14:30 sample_data\n",
            "drwxr-xr-x 2 root root 4096 Nov 23 13:39 scripts\n",
            "drwxr-xr-x 2 root root 4096 Nov 23 13:39 src\n"
          ]
        }
      ]
    },
    {
      "id": "9f9e7d1d",
      "cell_type": "markdown",
      "source": [
        "## 2) Install required packages\n",
        "Run this cell to install transformers, datasets, and PyTorch (Colab often already has torch)."
      ],
      "metadata": {
        "id": "9f9e7d1d"
      }
    },
    {
      "id": "3fb17290",
      "cell_type": "code",
      "metadata": {
        "id": "3fb17290"
      },
      "execution_count": null,
      "source": [
        "!pip install -q transformers datasets seqeval tokenizers accelerate sacrebleu\n",
        "!pip install -q torch\n"
      ],
      "outputs": []
    },
    {
      "id": "c648088a",
      "cell_type": "markdown",
      "source": [
        "## 3) Write the dataset generator script\n",
        "This script creates `data/train.jsonl` and `data/dev.jsonl` with noisy STT patterns."
      ],
      "metadata": {
        "id": "c648088a"
      }
    },
    {
      "id": "56cf48cf",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56cf48cf",
        "outputId": "01aa25ea-3a4e-4232-ede6-be9d031ea592"
      },
      "execution_count": null,
      "source": [
        "%%writefile scripts/generate_stt_data.py\n",
        "import json\n",
        "import random\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "NAMES = [\"john\", \"mary\", \"alice\", \"bob\", \"mohammed\", \"anvit\", \"ravi\", \"sneha\", \"arjun\", \"kiran\"]\n",
        "DOMAINS = [\"gmail\", \"yahoo\", \"example\", \"outlook\"]\n",
        "CITIES = [\"mumbai\", \"chennai\", \"delhi\", \"bangalore\", \"kolkata\"]\n",
        "FILLERS = [\"uh\", \"okay\", \"you know\", \"like\"]\n",
        "\n",
        "MAP_DIGIT = {\n",
        "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
        "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
        "}\n",
        "\n",
        "def random_name():\n",
        "    name = random.choice(NAMES)\n",
        "    if random.random() < 0.25:\n",
        "        sec = random.choice(NAMES)\n",
        "        if sec != name:\n",
        "            name = name + \" \" + sec\n",
        "    return name\n",
        "\n",
        "def make_email():\n",
        "    local = random.choice(NAMES)\n",
        "    if random.random() < 0.2:\n",
        "        local = \" \".join(list(local))\n",
        "    local = local.replace(\" \", \" dot \")\n",
        "    return f\"{local} at {random.choice(DOMAINS)} dot com\", \"EMAIL\"\n",
        "\n",
        "def make_phone():\n",
        "    digits = ''.join(random.choice('0123456789') for _ in range(10))\n",
        "    if random.random() < 0.5:\n",
        "        return \" \".join(MAP_DIGIT[d] for d in digits), \"PHONE\"\n",
        "    return digits, \"PHONE\"\n",
        "\n",
        "def make_credit():\n",
        "    digits = ''.join(random.choice('0123456789') for _ in range(16))\n",
        "    return \" \".join(MAP_DIGIT[d] for d in digits), \"CREDIT_CARD\"\n",
        "\n",
        "def make_date():\n",
        "    day = random.randint(1, 28)\n",
        "    month = random.choice([\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
        "                           \"july\",\"august\",\"september\",\"october\",\"november\",\"december\"])\n",
        "    year = random.choice([\"two thousand twenty\", \"twenty twenty one\"])\n",
        "    return f\"{day} of {month} {year}\", \"DATE\"\n",
        "\n",
        "def make_city():\n",
        "    return random.choice(CITIES), \"CITY\"\n",
        "\n",
        "ENTITY_MAKERS = [make_email, make_phone, make_credit, make_date, random_name, make_city]\n",
        "\n",
        "def generate_example():\n",
        "    base = [\"i\", \"think\", \"my\", \"account\", \"is\", \"with\"]\n",
        "    if random.random() < 0.3:\n",
        "        base.insert(0, random.choice(FILLERS))\n",
        "    text = \" \".join(base)\n",
        "\n",
        "    entities = []\n",
        "    maker = random.choice(ENTITY_MAKERS)\n",
        "    if maker == random_name:\n",
        "        ent_text = maker()\n",
        "        label = \"PERSON_NAME\"\n",
        "    else:\n",
        "        ent_text, label = maker()\n",
        "    text = text + \" \" + ent_text\n",
        "    start = text.find(ent_text)\n",
        "    end = start + len(ent_text)\n",
        "\n",
        "    return {\"id\": str(uuid.uuid4())[:8], \"text\": text, \"entities\": [{\"start\": start, \"end\": end, \"label\": label}]}\n",
        "\n",
        "def write_dataset():\n",
        "    Path(\"data\").mkdir(exist_ok=True)\n",
        "    with open(\"data/train.jsonl\", \"w\") as t:\n",
        "        for _ in range(800):\n",
        "            t.write(json.dumps(generate_example()) + \"\\n\")\n",
        "    with open(\"data/dev.jsonl\", \"w\") as d:\n",
        "        for _ in range(150):\n",
        "            d.write(json.dumps(generate_example()) + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(42)\n",
        "    write_dataset()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/generate_stt_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/generate_stt_data.py\n",
        "!head -n 5 data/train.jsonl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cvNd-T98pwI",
        "outputId": "ebb7b6d4-7be7-448c-e9df-507c200d3fdd"
      },
      "id": "_cvNd-T98pwI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"3e4693b1\", \"text\": \"i think my account is with mohammed at yahoo dot com\", \"entities\": [{\"start\": 27, \"end\": 52, \"label\": \"EMAIL\"}]}\n",
            "{\"id\": \"b4c67348\", \"text\": \"i think my account is with kolkata\", \"entities\": [{\"start\": 27, \"end\": 34, \"label\": \"CITY\"}]}\n",
            "{\"id\": \"585675aa\", \"text\": \"like i think my account is with j dot o dot h dot n at yahoo dot com\", \"entities\": [{\"start\": 32, \"end\": 68, \"label\": \"EMAIL\"}]}\n",
            "{\"id\": \"5978bfb9\", \"text\": \"i think my account is with a dot r dot j dot u dot n at outlook dot com\", \"entities\": [{\"start\": 27, \"end\": 71, \"label\": \"EMAIL\"}]}\n",
            "{\"id\": \"4ff6683b\", \"text\": \"you know i think my account is with alice at example dot com\", \"entities\": [{\"start\": 36, \"end\": 60, \"label\": \"EMAIL\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/dataset.py\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "LABEL_LIST = [\n",
        "    \"O\",\n",
        "    \"B-CREDIT_CARD\",\"I-CREDIT_CARD\",\n",
        "    \"B-PHONE\",\"I-PHONE\",\n",
        "    \"B-EMAIL\",\"I-EMAIL\",\n",
        "    \"B-PERSON_NAME\",\"I-PERSON_NAME\",\n",
        "    \"B-DATE\",\"I-DATE\",\n",
        "    \"B-CITY\",\"I-CITY\",\n",
        "]\n",
        "\n",
        "label_to_id = {l:i for i,l in enumerate(LABEL_LIST)}\n",
        "\n",
        "def read_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path,\"r\")]\n",
        "\n",
        "def char_labels_to_token_labels(tokenizer, text, entities):\n",
        "    enc = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    labels = [\"O\"] * len(offsets)\n",
        "\n",
        "    for ent in entities:\n",
        "        s, e, lab = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
        "        started = False\n",
        "        for i,(os,oe) in enumerate(offsets):\n",
        "            if oe<=s or os>=e:\n",
        "                continue\n",
        "            if not started:\n",
        "                labels[i] = \"B-\"+lab\n",
        "                started = True\n",
        "            else:\n",
        "                labels[i] = \"I-\"+lab\n",
        "\n",
        "    return enc, [label_to_id[l] for l in labels]\n",
        "\n",
        "def build_hf_dataset(tokenizer, path):\n",
        "    data = read_jsonl(path)\n",
        "    inputs=[]; masks=[]; lbls=[]; offs=[]; texts=[]\n",
        "    for item in data:\n",
        "        enc, ids = char_labels_to_token_labels(tokenizer,item[\"text\"],item[\"entities\"])\n",
        "        inputs.append(enc[\"input_ids\"])\n",
        "        masks.append(enc[\"attention_mask\"])\n",
        "        lbls.append(ids)\n",
        "        offs.append(enc[\"offset_mapping\"])\n",
        "        texts.append(item[\"text\"])\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"input_ids\":inputs,\n",
        "        \"attention_mask\":masks,\n",
        "        \"labels\":lbls,\n",
        "        \"offset_mapping\":offs,\n",
        "        \"text\":texts\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeSEYll99oRF",
        "outputId": "c4fcee25-a535-4c44-e9c7-938b360e0ce9"
      },
      "id": "DeSEYll99oRF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train.py\n",
        "import argparse\n",
        "from transformers import AutoTokenizer,AutoModelForTokenClassification,TrainingArguments,Trainer\n",
        "from src.dataset import build_hf_dataset, LABEL_LIST\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_name\",default=\"distilbert-base-uncased\")\n",
        "    parser.add_argument(\"--train\",default=\"data/train.jsonl\")\n",
        "    parser.add_argument(\"--dev\",default=\"data/dev.jsonl\")\n",
        "    parser.add_argument(\"--out_dir\",default=\"out/distil_baseline\")\n",
        "    parser.add_argument(\"--epochs\",type=int,default=2)\n",
        "    parser.add_argument(\"--batch_size\",type=int,default=8)\n",
        "    args=parser.parse_args()\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "    train = build_hf_dataset(tok, args.train)\n",
        "    dev = build_hf_dataset(tok, args.dev)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        args.model_name,\n",
        "        num_labels=len(LABEL_LIST)\n",
        "    )\n",
        "\n",
        "    tr_args = TrainingArguments(\n",
        "        output_dir=args.out_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.epochs,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=50\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=tr_args,\n",
        "        train_dataset=train,\n",
        "        eval_dataset=dev,\n",
        "        tokenizer=tok,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.out_dir)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P76dfQ_9vRS",
        "outputId": "52b0fe40-2752-4d81-9aff-ed48dbdaef1c"
      },
      "id": "7P76dfQ_9vRS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train.py\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "sys.path.append('/content/src')\n",
        "\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from src.dataset import build_hf_dataset, LABEL_LIST\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_name\", default=\"distilbert-base-uncased\")\n",
        "    parser.add_argument(\"--train\", default=\"data/train.jsonl\")\n",
        "    parser.add_argument(\"--dev\", default=\"data/dev.jsonl\")\n",
        "    parser.add_argument(\"--out_dir\", default=\"out/distil_baseline\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "    train_ds = build_hf_dataset(tokenizer, args.train)\n",
        "    dev_ds = build_hf_dataset(tokenizer, args.dev)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        args.model_name,\n",
        "        num_labels=len(LABEL_LIST)\n",
        "    )\n",
        "\n",
        "    # üî• FIX: Automatic padding for input + labels\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.out_dir,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.epochs,\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        logging_dir=\"logs\",\n",
        "        do_eval=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=dev_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator     # ‚≠ê VERY IMPORTANT\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.out_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEs8oqFi-6U5",
        "outputId": "5852eaae-d9bd-4d3f-ae5f-8bc904d62ba1"
      },
      "id": "JEs8oqFi-6U5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "r1JTzDi7BEY1"
      },
      "id": "r1JTzDi7BEY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train.py --epochs 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSY2tQj09-_r",
        "outputId": "fb50f492-ea01-4228-ad95-517e0412d358"
      },
      "id": "rSY2tQj09-_r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-23 14:01:15.031634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763906475.108488    5743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763906475.150423    5743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763906475.207019    5743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763906475.207096    5743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763906475.207105    5743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763906475.207114    5743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 14:01:15.227327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/content/src/train.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "  0% 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "{'loss': 0.4777, 'grad_norm': 0.8143473267555237, 'learning_rate': 3.775e-05, 'epoch': 0.5}\n",
            "{'loss': 0.024, 'grad_norm': 0.0710238367319107, 'learning_rate': 2.525e-05, 'epoch': 1.0}\n",
            "{'loss': 0.0059, 'grad_norm': 0.04985155537724495, 'learning_rate': 1.2750000000000002e-05, 'epoch': 1.5}\n",
            "{'loss': 0.0055, 'grad_norm': 0.031964775174856186, 'learning_rate': 2.5000000000000004e-07, 'epoch': 2.0}\n",
            "{'train_runtime': 298.9157, 'train_samples_per_second': 5.353, 'train_steps_per_second': 0.669, 'train_loss': 0.12829396665096282, 'epoch': 2.0}\n",
            "100% 200/200 [04:58<00:00,  1.49s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R out/distil_baseline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFhaZ83--j5n",
        "outputId": "fd8eb868-1ced-44ee-c3c1-d07057f04039"
      },
      "id": "VFhaZ83--j5n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out/distil_baseline:\n",
            "checkpoint-200\t   special_tokens_map.json  training_args.bin\n",
            "config.json\t   tokenizer_config.json    vocab.txt\n",
            "model.safetensors  tokenizer.json\n",
            "\n",
            "out/distil_baseline/checkpoint-200:\n",
            "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
            "model.safetensors  special_tokens_map.json  training_args.bin\n",
            "optimizer.pt\t   tokenizer_config.json    vocab.txt\n",
            "rng_state.pth\t   tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/predict_and_postprocess.py\n",
        "import json, re, argparse, torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from src.dataset import LABEL_LIST\n",
        "\n",
        "EMAIL_RE = re.compile(r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\")\n",
        "\n",
        "def normalize(text):\n",
        "    return text.replace(\" at \", \"@\").replace(\" dot \", \".\")\n",
        "\n",
        "def bio_to_spans(tokens, labels, offsets, text):\n",
        "    spans = []\n",
        "    curr = None\n",
        "    for i, lid in enumerate(labels):\n",
        "        lab = LABEL_LIST[lid]\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if curr:\n",
        "                spans.append(curr)\n",
        "            curr = {\"label\": lab[2:], \"start\": offsets[i][0], \"end\": offsets[i][1]}\n",
        "        elif lab.startswith(\"I-\") and curr:\n",
        "            curr[\"end\"] = offsets[i][1]\n",
        "        else:\n",
        "            if curr:\n",
        "                spans.append(curr)\n",
        "                curr = None\n",
        "    if curr:\n",
        "        spans.append(curr)\n",
        "\n",
        "    for s in spans:\n",
        "        s[\"text\"] = text[s[\"start\"]:s[\"end\"]]\n",
        "    return spans\n",
        "\n",
        "def is_valid(span):\n",
        "    if span[\"label\"] == \"EMAIL\":\n",
        "        return EMAIL_RE.match(normalize(span[\"text\"])) is not None\n",
        "    return True\n",
        "\n",
        "def run_prediction(model_dir=\"out/distil_baseline\",\n",
        "                   input_path=\"data/dev.jsonl\",\n",
        "                   output_path=\"out/dev_pred.json\"):\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    data = [json.loads(l) for l in open(input_path)]\n",
        "    out = []\n",
        "\n",
        "    for item in data:\n",
        "        text = item[\"text\"]\n",
        "        enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits.argmax(-1)[0]\n",
        "\n",
        "        spans = bio_to_spans(\n",
        "            tok.convert_ids_to_tokens(enc[\"input_ids\"][0]),\n",
        "            pred.tolist(),\n",
        "            enc[\"offset_mapping\"][0].tolist(),\n",
        "            text\n",
        "        )\n",
        "\n",
        "        final = [s for s in spans if is_valid(s)]\n",
        "        for f in final:\n",
        "            f[\"pii\"] = True\n",
        "\n",
        "        out.append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"text\": text,\n",
        "            \"predicted_entities\": final\n",
        "        })\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        for x in out:\n",
        "            f.write(json.dumps(x) + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_dir\", default=\"out/distil_baseline\")\n",
        "    parser.add_argument(\"--input\", default=\"data/dev.jsonl\")\n",
        "    parser.add_argument(\"--output\", default=\"out/dev_pred.json\")\n",
        "    args = parser.parse_args()\n",
        "    run_prediction(args.model_dir, args.input, args.output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAC4Q8s7D4zx",
        "outputId": "e0140cab-2e78-4ba0-8e79-582c5dfb9c8b"
      },
      "id": "PAC4Q8s7D4zx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/predict_and_postprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,200p' src/predict_and_postprocess.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qjevu69EmxX",
        "outputId": "8297d20a-0c25-4e33-804d-ceace69c3e19"
      },
      "id": "4qjevu69EmxX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import json, re, argparse, torch\n",
            "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
            "from src.dataset import LABEL_LIST\n",
            "\n",
            "EMAIL_RE = re.compile(r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\")\n",
            "\n",
            "def normalize(text):\n",
            "    return text.replace(\" at \", \"@\").replace(\" dot \", \".\")\n",
            "\n",
            "def bio_to_spans(tokens, labels, offsets, text):\n",
            "    spans = []\n",
            "    curr = None\n",
            "    for i, lid in enumerate(labels):\n",
            "        lab = LABEL_LIST[lid]\n",
            "        if lab.startswith(\"B-\"):\n",
            "            if curr:\n",
            "                spans.append(curr)\n",
            "            curr = {\"label\": lab[2:], \"start\": offsets[i][0], \"end\": offsets[i][1]}\n",
            "        elif lab.startswith(\"I-\") and curr:\n",
            "            curr[\"end\"] = offsets[i][1]\n",
            "        else:\n",
            "            if curr:\n",
            "                spans.append(curr)\n",
            "                curr = None\n",
            "    if curr:\n",
            "        spans.append(curr)\n",
            "\n",
            "    for s in spans:\n",
            "        s[\"text\"] = text[s[\"start\"]:s[\"end\"]]\n",
            "    return spans\n",
            "\n",
            "def is_valid(span):\n",
            "    if span[\"label\"] == \"EMAIL\":\n",
            "        return EMAIL_RE.match(normalize(span[\"text\"])) is not None\n",
            "    return True\n",
            "\n",
            "def run_prediction(model_dir=\"out/distil_baseline\",\n",
            "                   input_path=\"data/dev.jsonl\",\n",
            "                   output_path=\"out/dev_pred.json\"):\n",
            "\n",
            "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
            "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
            "    model.eval()\n",
            "\n",
            "    data = [json.loads(l) for l in open(input_path)]\n",
            "    out = []\n",
            "\n",
            "    for item in data:\n",
            "        text = item[\"text\"]\n",
            "        enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
            "\n",
            "        with torch.no_grad():\n",
            "            pred = model(enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits.argmax(-1)[0]\n",
            "\n",
            "        spans = bio_to_spans(\n",
            "            tok.convert_ids_to_tokens(enc[\"input_ids\"][0]),\n",
            "            pred.tolist(),\n",
            "            enc[\"offset_mapping\"][0].tolist(),\n",
            "            text\n",
            "        )\n",
            "\n",
            "        final = [s for s in spans if is_valid(s)]\n",
            "        for f in final:\n",
            "            f[\"pii\"] = True\n",
            "\n",
            "        out.append({\n",
            "            \"id\": item[\"id\"],\n",
            "            \"text\": text,\n",
            "            \"predicted_entities\": final\n",
            "        })\n",
            "\n",
            "    with open(output_path, \"w\") as f:\n",
            "        for x in out:\n",
            "            f.write(json.dumps(x) + \"\\n\")\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser()\n",
            "    parser.add_argument(\"--model_dir\", default=\"out/distil_baseline\")\n",
            "    parser.add_argument(\"--input\", default=\"data/dev.jsonl\")\n",
            "    parser.add_argument(\"--output\", default=\"out/dev_pred.json\")\n",
            "    args = parser.parse_args()\n",
            "    run_prediction(args.model_dir, args.input, args.output)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/src/__pycache__/*.pyc\n",
        "!rm -rf /content/src/__pycache__\n"
      ],
      "metadata": {
        "id": "Gxf4HgBaF9qB"
      },
      "id": "Gxf4HgBaF9qB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content\")\n",
        "sys.path.append(\"/content/src\")\n",
        "\n",
        "import importlib\n",
        "import src.predict_and_postprocess\n",
        "importlib.reload(src.predict_and_postprocess)\n",
        "\n",
        "from src.predict_and_postprocess import run_prediction\n"
      ],
      "metadata": {
        "id": "rMyltZ-DEmvJ"
      },
      "id": "rMyltZ-DEmvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_prediction()\n",
        "\n",
        "!head -n 10 out/dev_pred.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE090m_OEmro",
        "outputId": "8dddc978-9c83-4cc0-aa1a-3fa785bdece2"
      },
      "id": "tE090m_OEmro",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"ead8ce41\", \"text\": \"like i think my account is with 0064256908\", \"predicted_entities\": [{\"label\": \"PHONE\", \"start\": 32, \"end\": 42, \"text\": \"0064256908\", \"pii\": true}]}\n",
            "{\"id\": \"33eb1887\", \"text\": \"i think my account is with 14 of july two thousand twenty\", \"predicted_entities\": [{\"label\": \"DATE\", \"start\": 27, \"end\": 57, \"text\": \"14 of july two thousand twenty\", \"pii\": true}]}\n",
            "{\"id\": \"145ef083\", \"text\": \"i think my account is with ravi at outlook dot com\", \"predicted_entities\": [{\"label\": \"EMAIL\", \"start\": 27, \"end\": 50, \"text\": \"ravi at outlook dot com\", \"pii\": true}]}\n",
            "{\"id\": \"72f26aa4\", \"text\": \"you know i think my account is with four six zero one five eight five one seven four three two eight zero six three\", \"predicted_entities\": [{\"label\": \"CREDIT_CARD\", \"start\": 36, \"end\": 115, \"text\": \"four six zero one five eight five one seven four three two eight zero six three\", \"pii\": true}]}\n",
            "{\"id\": \"7c3a5eac\", \"text\": \"like i think my account is with anvit at outlook dot com\", \"predicted_entities\": [{\"label\": \"EMAIL\", \"start\": 32, \"end\": 56, \"text\": \"anvit at outlook dot com\", \"pii\": true}]}\n",
            "{\"id\": \"556a5036\", \"text\": \"i think my account is with zero three zero nine three seven six one eight four\", \"predicted_entities\": [{\"label\": \"PHONE\", \"start\": 27, \"end\": 78, \"text\": \"zero three zero nine three seven six one eight four\", \"pii\": true}]}\n",
            "{\"id\": \"4c551a27\", \"text\": \"i think my account is with 1 of january twenty twenty one\", \"predicted_entities\": [{\"label\": \"DATE\", \"start\": 27, \"end\": 57, \"text\": \"1 of january twenty twenty one\", \"pii\": true}]}\n",
            "{\"id\": \"fec2107d\", \"text\": \"uh i think my account is with 4 of april twenty twenty one\", \"predicted_entities\": [{\"label\": \"DATE\", \"start\": 30, \"end\": 58, \"text\": \"4 of april twenty twenty one\", \"pii\": true}]}\n",
            "{\"id\": \"cb6c36b4\", \"text\": \"i think my account is with alice\", \"predicted_entities\": [{\"label\": \"PERSON_NAME\", \"start\": 27, \"end\": 32, \"text\": \"alice\", \"pii\": true}]}\n",
            "{\"id\": \"c8d36554\", \"text\": \"uh i think my account is with alice at gmail dot com\", \"predicted_entities\": [{\"label\": \"EMAIL\", \"start\": 30, \"end\": 52, \"text\": \"alice at gmail dot com\", \"pii\": true}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: compute PII metrics (dev)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content\")\n",
        "gold_path = BASE/\"data\"/\"dev.jsonl\"\n",
        "pred_path = BASE/\"out\"/\"dev_pred.json\"\n",
        "\n",
        "def load_jsonl(p):\n",
        "    if not p.exists():\n",
        "        print(\"MISSING:\", p)\n",
        "        return None\n",
        "    return [json.loads(l) for l in open(p, \"r\", encoding=\"utf8\")]\n",
        "\n",
        "gold = load_jsonl(gold_path)\n",
        "pred = load_jsonl(pred_path)\n",
        "\n",
        "if gold is None or pred is None:\n",
        "    raise SystemExit(\"Gold or prediction file missing. Check paths and generate predictions first.\")\n",
        "\n",
        "gold_map = {g[\"id\"]: g for g in gold}\n",
        "pred_map = {p[\"id\"]: p for p in pred}\n",
        "\n",
        "labels_to_check = {\"CREDIT_CARD\",\"PHONE\",\"EMAIL\",\"PERSON_NAME\",\"DATE\"}\n",
        "tp = fp = fn = 0\n",
        "for gid, g in gold_map.items():\n",
        "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in g.get(\"entities\", []) if e[\"label\"] in labels_to_check}\n",
        "    p = pred_map.get(gid, {})\n",
        "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in p.get(\"predicted_entities\", []) if e[\"label\"] in labels_to_check}\n",
        "    for ps in pred_spans:\n",
        "        if ps in gold_spans:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "    for gs in gold_spans:\n",
        "        if gs not in pred_spans:\n",
        "            fn += 1\n",
        "\n",
        "precision = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
        "recall = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
        "f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
        "\n",
        "metrics = {\"TP\":tp,\"FP\":fp,\"FN\":fn,\"precision\":round(precision,4),\"recall\":round(recall,4),\"f1\":round(f1,4)}\n",
        "print(\"Metrics:\", json.dumps(metrics, indent=2))\n",
        "\n",
        "# Save metrics to file\n",
        "outdir = Path(\"/mnt/data\")\n",
        "outdir.mkdir(exist_ok=True)\n",
        "with open(outdir/\"metrics.json\",\"w\",encoding=\"utf8\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(\"Saved metrics to /mnt/data/metrics.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0BNbZ5UFKmv",
        "outputId": "c5bdeeee-60d2-4655-ea7b-240a04acd8fd"
      },
      "id": "n0BNbZ5UFKmv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: {\n",
            "  \"TP\": 126,\n",
            "  \"FP\": 0,\n",
            "  \"FN\": 0,\n",
            "  \"precision\": 1.0,\n",
            "  \"recall\": 1.0,\n",
            "  \"f1\": 1.0\n",
            "}\n",
            "Saved metrics to /mnt/data/metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: create submission zip\n",
        "import zipfile, os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content\")\n",
        "OUTZIP = Path(\"/mnt/data/PII_NER_submission.zip\")\n",
        "ASSIGNMENT_PDF = Path(\"/mnt/data/IIT Madras __ Assignment 2025 (1).pdf\")\n",
        "\n",
        "with zipfile.ZipFile(OUTZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    # add src\n",
        "    if (BASE/\"src\").exists():\n",
        "        for root,_,files in os.walk(BASE/\"src\"):\n",
        "            for fn in files:\n",
        "                full = Path(root)/fn\n",
        "                z.write(full, arcname=str(Path(\"src\")/full.relative_to(BASE/\"src\")))\n",
        "    # add scripts\n",
        "    if (BASE/\"scripts\").exists():\n",
        "        for root,_,files in os.walk(BASE/\"scripts\"):\n",
        "            for fn in files:\n",
        "                full = Path(root)/fn\n",
        "                z.write(full, arcname=str(Path(\"scripts\")/full.relative_to(BASE/\"scripts\")))\n",
        "    # add prediction if exists\n",
        "    pred = BASE/\"out\"/\"dev_pred.json\"\n",
        "    if pred.exists():\n",
        "        z.write(pred, arcname=\"out/dev_pred.json\")\n",
        "    # add README and metrics\n",
        "    for fn in [\"README.md\",\"metrics.json\"]:\n",
        "        p = Path(\"/mnt/data\")/fn\n",
        "        if p.exists():\n",
        "            z.write(p, arcname=fn)\n",
        "    # add assignment PDF if exists\n",
        "    if ASSIGNMENT_PDF.exists():\n",
        "        z.write(ASSIGNMENT_PDF, arcname=ASSIGNMENT_PDF.name)\n",
        "\n",
        "print(\"Created:\", OUTZIP)\n",
        "print(\"Size (bytes):\", OUTZIP.stat().st_size)\n",
        "print(\"List contents (first 30):\")\n",
        "import zipfile\n",
        "with zipfile.ZipFile(OUTZIP, \"r\") as z:\n",
        "    for info in z.infolist()[:30]:\n",
        "        print(\"-\", info.filename, info.file_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfM834xCI0dv",
        "outputId": "774d415c-da13-497a-960f-8d986d5834a8"
      },
      "id": "tfM834xCI0dv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: /mnt/data/PII_NER_submission.zip\n",
            "Size (bytes): 11761\n",
            "List contents (first 30):\n",
            "- src/dataset.py 1572\n",
            "- src/predict_and_postprocess.py 2585\n",
            "- src/train.py 1873\n",
            "- src/__pycache__/predict_and_postprocess.cpython-312.pyc 4400\n",
            "- scripts/generate_stt_data.py 2721\n",
            "- out/dev_pred.json 32012\n",
            "- README.md 1680\n",
            "- metrics.json 87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7EduzHENMv-R"
      },
      "id": "7EduzHENMv-R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /mnt/data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoUm_c5nI7Ef",
        "outputId": "fee99d6c-be11-4328-b8f1-e58e9b38c04f"
      },
      "id": "yoUm_c5nI7Ef",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20K\n",
            "-rw-r--r-- 1 root root   87 Nov 23 14:33 metrics.json\n",
            "-rw-r--r-- 1 root root  12K Nov 23 14:34 PII_NER_submission.zip\n",
            "-rw-r--r-- 1 root root 1.7K Nov 23 14:34 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /mnt/data/PII_NER_submission.zip /content/\n",
        "!cp /mnt/data/metrics.json /content/\n",
        "!cp /mnt/data/README.md /content/\n",
        "!cp /content/out/dev_pred.json /content/\n"
      ],
      "metadata": {
        "id": "0BSlZpELNKIl"
      },
      "id": "0BSlZpELNKIl",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Og6eSCQBOgnv"
      },
      "id": "Og6eSCQBOgnv",
      "execution_count": null,
      "outputs": []
    }
  ]
}